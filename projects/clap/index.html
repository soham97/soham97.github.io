<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Audio concepts from language | Soham Deshmukh</title> <meta name="author" content="Soham Deshmukh"> <meta name="description" content="Learning audio concepts from natural language supervision"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, soham deshmukh, microsoft, cmu, audio, speech"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9D%84%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://soham97.github.io/projects/clap/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Soham </span>Deshmukh</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">talks &amp; teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Audio concepts from language</h1> <p class="post-description">Learning audio concepts from natural language supervision</p> </header> <article> <p><a href="https://www.microsoft.com/en-us/research/project/audio-analytics/" rel="external nofollow noopener" target="_blank">Audio Analytics</a> focuses on analyzing and understanding audio signals. This technology has a variety of applications in context-based indexing, retrieval in multimedia databases, unobtrusive monitoring in health care, surveillance, and makes products like Amazon Echo, and Apple home intelligent.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/clap_applications.jpg" alt="" title="example image"> </div> </div> <div class="caption"> Applications of audio analytics technology </div> <p>The north star goal of the audio analytics technology is to achieve human ear understanding and performance. To achieve this goal, the mainstream Machine Learning (ML) approaches for audio understanding break the human hearing into smaller tasks such as sound event detection, acoustic scene classification, emotion recognition, etc. Creating ML models for the smaller task requires defining a unique label ontology, followed by data collection and model training phase. This time-consuming process has to be repeated for each task.</p> <p>Therefore in our recent work [1], we ask the question <em>“Can we learn audio concepts directly from natural language supervision?”</em>. We introduce Contrastive Language-Audio Pretraining (CLAP <img class="emoji" title=":clap:" alt=":clap:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f44f.png" height="20" width="20">), which learns to connect language and audio by jointly learning a multimodal space using contrastive learning.</p> <p>The benefits of the proposed natural language learning supervision are:</p> <ul> <li> <strong>Foundation Model.</strong> This learning framework allows the training of a foundation audio model for audio for a large web corpus of audio-text pairs. The trained foundation model can then power multiple downstream audio tasks with little or no finetuning on the target domain. An equivalent example of such models can be found in the field of computer vision (Open AI CLIP, Microsoft Florence) and language (BERT, PaLM, GPT3)</li> <li> <strong>Zero-Shot capability.</strong> The current large-scale pre-trained models still require an additional layer of finetuning on the target dataset. This includes Self-Supervised Learning (SSL) models for Speech and Audio. However, CLAP does not require the additional step of finetuning required on the target dataset. The model enables Zero-Shot (ZS) predictions and takes input audio and yields a prediction score for any class typed by the user.</li> </ul> <p>To show the potential of audio-text supervision we collect 128k audio-text pairs from audio captioning and sound event datasets. Some examples of what the audio-text pairs look like are below:</p> <div class="row"> <audio controls=""> <source src="/assets/audio/Media1.mp3" type="audio/mp3"></source> Your browser does not support the audio element. </audio> <div class="caption"> String instrument playing one tone repeatedly before voilin joins to create melody </div> <br> <audio controls=""> <source src="/assets/audio/Media2.mp3" type="audio/mp3"></source> Your browser does not support the audio element. </audio> <div class="caption"> An insect buzzing in the foreground as birds chirps in the background </div> <audio controls=""> <source src="/assets/audio/Media3.mp3" type="audio/mp3"></source> Your browser does not support the audio element. </audio> <div class="caption"> A campfire crackles as the flame burns branches and leaves </div> </div> <div class="caption"> Three randomly sampled audio-text pairs </div> <p>For training, CLAP jointly trains an audio and a text encoder to learn the (dis)similarity of audio and text pairs in a batch using contrastive learning. At testing time, the pretrained encoders are used to extract audio embeddings from the testing audio and text embeddings from the class labels. Zero-Shot linear classification is achieved by computing cosine similarity between the embeddings.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/clap.jpg" alt="" title="example image"> </div> </div> <div class="caption"> Training and Zero-Shot evaluation of CLAP </div> <p>Once CLAP is trained, it can be used to provide Zero-Shot inference in a few lines of code. We evaluate CLAP’s ZS performance across 16 datasets from 8 domains as downstream tasks. We also evaluate CLAP in the traditional supervised setup on the downstream dataset. This traditional setup is shown as <em>CLAP (Best)</em> in the table below:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/clap_zs.jpg" alt="" title="example image"> </div> </div> <div class="caption"> CLAP (ZS) Zero-Shot outperforms the literature. CLAP (Best) is the best performance among our supervised setups. Higher is better for all numbers, DCASE17 employs F1, FSD50K and AudioSet employs mAP, everything else uses accuracy </div> <p>The paper [1] covers the analysis of results, ablation studies for training, and the effects of prompt tuning.</p> <p>Overall, CLAP shows excellent promise. I am personally excited to see the upcoming research in this direction and the emergence of new audio applications powered by such models.</p> <p>Twitter conversations about CLAP:</p> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">CLAP: Learning Audio Concepts From Natural Language Supervision<br>abs: <a href="https://t.co/cZX7fJ7ifs" rel="external nofollow noopener" target="_blank">https://t.co/cZX7fJ7ifs</a><br><br>trained CLAP with 128k audio and text pairs and evaluated it on 16 downstream tasks across 8 domains, such as Sound Event Classification, Music tasks, and Speech-related tasks <a href="https://t.co/Y8lVGPazIV" rel="external nofollow noopener" target="_blank">pic.twitter.com/Y8lVGPazIV</a></p>— AK (@_akhaliq) <a href="https://twitter.com/_akhaliq/status/1536495422683893765?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">June 13, 2022</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <h2 id="references">References</h2> <p>[1] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, Huaming Wang, “CLAP: Learning Audio Concepts from Natural Language Supervision,” ArXiv 2022</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Soham Deshmukh. Last updated: June 10, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-127734545-1"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-127734545-1");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>