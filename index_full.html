<meta name="viewport" content="width=device-width, initial-scale=1">
<html> 
<head> 
  <link type="text/css"
        rel="stylesheet"
        href="style.css">
<title>Soham Deshmukh</title> 
</head> 
<body> 
<h1>Soham Deshmukh</h1> 
<div style="display:grid;grid-template-columns: 80% 20%;">
  <div style="float:left;">
    <p>
    I'm an Applied Scientist on the <a href="https://www.microsoft.com/en-us/research/people/sdeshmukh/">Microsoft Speech team</a>, working on speech and audio processing. 
    </p>
    <p>
    My work focuses on building audio processing technology to reduce communication barriers and enabling seamless interactions. 
    This ranges from front-end audio processing like speech enhancement to building general purpose audio assistants.  
    My research gets deployed in products like <a href="https://www.microsoft.com/en-us/microsoft-teams/group-chat-software">Teams</a>, <a href="https://www.microsoft.com/en-us/edge">Edge</a>, <a href="https://www.microsoft.com/en-us/microsoft-365/outlook/email-and-calendar-software-microsoft-outlook">Outlook</a>.
    </p>
    <p>
    Previously, I received my masters degree from Carnegie Mellon University, <a href="https://cmu-mlsp.github.io/">MLSP Group</a> and advised by <a href="https://cmu-mlsp.github.io/team/bhiksha_raj">Bhiksha Raj</a>. 
    I completed my B.Tech from <a href="https://en.wikipedia.org/wiki/Veermata_Jijabai_Technological_Institute">VJTI</a>, working on NLP. 
    </p>
    <b>Collaborations</b>: If you have questions or want to collaborate with me, feel free to email me.
    </p>

    <p>
    <b>Links:</b>
    <a href="https://scholar.google.com/citations?user=MasiEogAAAAJ&hl=en" target="_blank">Google Scholar</a> &bull;
    <a href="https://github.com/soham97" target="_blank">GitHub</a> &bull;  
    <a href="https://x.com/sohamdesh_" target="_blank">Twitter</a> &bull;  
    <a href="https://www.linkedin.com/in/sdeshmuk/" target="_blank">LinkedIn</a>
    </p>

  </div>
  <div style="float:left">  
    <img class="mug center" style="max-width:85%" src="images/soham.jpg" alt="Soham Deshmukh"> 
  </div>

</div>

<hr>
<div class="navigation_list">
  <h2>
    <a href="#research_place">Research Topics</a> &bull;
    <a href="#news_place">Updates</a> &bull;
    <!-- <a href="#people_place">People</a> &bull; -->
    <a href="#papers_place">Papers</a> &bull;
    <a href="#press_place">Press</a> &bull;
    <a href="#service_place">Service</a> 
    <!-- &bull;<a href="#contact">Contact</a> -->
  </h2>
</div>

<div id="places">
<hr>

<div id="research_place">
  <h2>Research Highlights</h2>
  <table>
    <colgroup>
      <col style="max-width: 50%">
      <col style="max-width: 50%">
    </colgroup>
    <tbody>
    <tr>
      <td>
        <div class=highlights>
          <b>Front-end Audio Processing</b><br>
          <p>
            Audio processing on the edge/client side i.e. before audio is sent to the server for further processing. 
            This includes tasks such as speech enhancement, background estimation, echo cancellation:
          </p>
          <ul class=collapsed class=bulletless>
            <li><a href="https://www.microsoft.com/en-us/edge/features/real-time-video-translation?form=MA13FJ" target="_blank">Real-time video translation</a></li>
            <li><a href="https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/announcing-video-translation-amp-speech-translation-api/ba-p/4148007" target="_blank">Video Translation API</a></li>
          </ul>
        </div>
      </td>
      <td>
        <img class="side-image" src="images/translation.png">
      </td>
    </tr>

    <tr>
      <td>
        <div class=highlights>
          <b>Audio-Language Models</b><br>
          <p>
            Grounding the LLMs in audio modality will enable models which can perceive (hear), analyze, and interact with users through natural language. By grounding language in audio, we aim to learn a rich audio-text representation that can enable various multimodal applications like text queried- retrieval, captioning, generation and question-answering
          </p>
          <ul class=collapsed class=bulletless>
            <li><a href="https://arxiv.org/abs/2309.05767" target="_blank">Natural Language Supervision for General-Purpose Audio Representations</a> (ICASSP 2024)</li>
            <li><a href="https://arxiv.org/abs/2212.10481" target="_blank">Pengi üêß: An Audio Language Model for Audio Tasks</a> (NeurIPS 2024)</li>
            <li><a href="https://arxiv.org/abs/2206.04769" target="_blank">CLAP üëè: Learning Audio Concepts From Natural Language Supervision</a> (ICASSP 2023)</li>
          </ul>
        </div>
      </td>
      <td>
        <img class="side-image" src="images/alm2.png">
      </td>
    </tr>
    </tbody>
  </table>

</div>


<div id="news_place">
<h2>News</h2>
  <ul class="collapsed bulletless">
    <li> <b>Jun 2024:</b> <a href="https://www.microsoft.com/en-us/edge/features/real-time-video-translation?form=MA13FJ" target="_blank">Real-time</a> and <a href="https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/announcing-video-translation-amp-speech-translation-api/ba-p/4148007" target="_blank">batch video translation</a> launched</i>.</li>
    <li> <b>Jun 2024:</b> Three papers accepted at INTERSPEECH 2024</i>.</li>
    <li> <b>Dec 2023:</b> Three papers accepted at ICASSP 2024</i>.</li>
    <li> <b>Sep 2023:</b> üêß Pengi is accepted at NeurIPS 2023</i>.</li>
    <li> <b>Jul 2023:</b> üëè CLAP and multi-view emotion is accepted to ICASSP 2023</i>.</li>
    <li> <b>Jul 2023:</b> WavText5K is accepted to INTERSPEECH 2023</i>.</li>
    <li> <b>Mar 2023:</b> Two papers accepted at ICASSP 2023</i>.</li>
    <li> <b>Feb 2022:</b> Scheduler service integrated in M365 and reached general availability</i>.</li>
    <li> <b>Jun 2021:</b> Improving SED in noisy environments accepted at INTERSPEECH 2021</i>.</li>
    <li> <b>Jan 2021:</b> Two papers accepted at ICASSP 2021</i>.</li>
    <li> <b>Dec 2020:</b> Invited talk given at MIT CSAIL SLS group</i>.</li>
    <li> <b>Nov 2020:</b> Released sound event detection reading list <a href="https://github.com/soham97/awesome-sound_event_detection" target="_blank">here</a></i>.</li>
    <li> <b>Aug 2020:</b>	Had a great summer internship at Microsoft</i>.</li>
    <li> <b>Apr 2020:</b> Voice-based COVID-19 screening is covered in <a href="https://www.forbes.com/sites/marcwebertobias/2020/05/05/ai-and-medical-diagnostics-can-a-smartphone-app-detect-covid-19-from-speech-or-a-cough/#7a9114225436"  target="_blank">Forbes</a>, <a href="https://gizmodo-com.cdn.ampproject.org/c/s/gizmodo.com/researchers-built-an-app-that-aims-to-detect-covid-19-b-1842613139/amp" target="_blank">Gizmodo</a></i>.</li>
  </ul>
</div>
<hr>

<!-- <div id="people_place">
<h2 name="people" id="people">People</h2>
Here are the amazing students I'm currently (Fall 2023) working with!
<h3>Advisees</h3>
  <ul class="collapsed bulletless">
    <li><a href="https://jykoh.com/">JY Koh</a> (CMU PhD, with Russ Salakhutdinov)</li>
    <li><a href="https://andyjliu.github.io/">Andy Liu</a> (CMU PhD, with Mona Diab)</li>
    <li><a href="https://saujasv.github.io/">Saujas Vaduguru</a> (CMU PhD)</li>
    <li><a href="https://zorazrw.github.io/">Zora Wang</a> (CMU PhD, with Graham Neubig)</li>
    <li><a href="https://axie66.github.io/">Alex Xie</a> (CMU MLT, with Matt Gormley and Vincent Hellendoorn)</li>
  </ul>
  <img style="max-width:50%" src="images/group-2024.jpg" alt="Group photo"> 
<hr>
</div> -->

<div id="papers_place">
<h2>Papers</h2>

<ul class="bulletless">
  <div id="include-publications"></div>
</ul>

</div>
<hr>

<div id="press_place">
<h2>Press</h2>
<ul class="collapsed bulletless">
  <li><a href="https://unlocked.microsoft.com/bioacoustics/" target="_blank">Microsoft Unlocked</a> Audio AI used for bioacoustics in Amazon Rainforest</i>.</li>
  <li><a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-april-1-2024/" target="_blank"> Microsoft Research Blog</a> Research on Automated Audio Captioning featured in Microsoft Research Blog</i>.</li>
  <li><a href="https://analyticsindiamag.com/microsoft-launches-pengi-an-audio-language-model-for-open-ended-tasks/" target="_blank"> Analytics India Magazine</a> Microsoft launches Pengi, an Audio Language Model for Open-ended Tasks</i>.</li>
  <li><a href="https://www.credly.com/badges/52315cc2-c5b5-46d1-a73b-e26f0b5899c3/public_url" target="_blank"> Microsoft Garage</a> Internal coverage for hackathon project in the area of speech and audio</i>.</li>
  <li><a href="https://www.businessinsider.com/ai-labs-diagnose-covid-19-voice-listening-talk-2020-4" target="_blank"> Business Insider</a> Do I sound sick to you? Researchers are building AI that would diagnose COVID-19 by listening to people talk</i>.</li>
  <li><a href="https://www.cbsnews.com/pittsburgh/news/carnegie-mellon-university-coronavirus-voice-app/" target="_blank"> Pittsburgh News</a> Coronavirus detected by voice? Carnegie Mellon researchers Develop app to ‚Äòlisten‚Äô for signs of COVID-19</i>.</li>
  <li><a href="https://www.forbes.com/sites/marcwebertobias/2020/05/05/ai-and-medical-diagnostics-can-a-smartphone-app-detect-covid-19-from-speech-or-a-cough/?sh=3aa0973f5436" target="_blank">Forbes</a> AI and medical diagnostics: can a smartphone app detect COVID-19 from speech or cough</i>?</li>
  <li><a href="https://www.indiatimes.com/technology/science-and-future/mumbai-s-oldest-tech-college-installs-ai-supercomputer-to-skill-students-attract-startups-372319.html" target="_blank"> Indiatimes</a> News coverage of Deepfake efforts in VJTI CoE-CNDS</i>.</li>
  <li><a href="https://dnif.it/ai-ml-cyber-security/blog/modelling-attackers-behavioral-patterns.html" target="_blank">DNIF</a> Modelling attacker behavioral patterns using statistical machine learning algorithms</i>.</li>
</ul>
</div>

<hr>

<div id="service_place">
<h2>Service</h2> 

<ul class="collapsed bulletless">
    <li>Reviewer for ICASSP, INTERSPEECH, NeurIPS, DCASE </li>
    <li>TA for Graph Signal Processing course, taught by Prof. Jose Moura (2024) </li>
    <li>TA for Machine Learning for Signal Processing course, taught by Prof. Bhiksha Raj (2023) </li>
    <li>Organized Special Session ‚ÄúSynergy between human and machine approaches to sound/scene recognition and processing‚Äù at ICASSP 2023</li>
    <li>TA for Introduction to Machine Learning course, taught by Prof. Gauri Joshi and Prof. Carlee Joe-Wong (2020) </li>
</ul>
</div>
<hr>

</div> <!-- end places -->

<script src="static/jquery.min.js"></script>
<script>
    var $j = jQuery;

    var tabIndex = 0;

    var sections = ["research", "news", "papers", "press", "service"];

    for(i=0; i<sections.length; i++) {
        $j("#"+sections[i]+"_place").hide();
      }

    $j('#places').find('hr').hide();
    $j('#places').find('h2').hide();


    ChooseList = function(node, changeCallback, place) {
        loadIndex = sections.indexOf(place) == -1 ? 0 : sections.indexOf(place);

        this.container = $j(node);
        this.selectedNode = null;
        this.currentIndex = null;
        this.onChange = changeCallback;
        this.elements = this.container.find('a');
        this.elements.removeAttr('target');
        this.container.find('a').on('click', $j.proxy(this.onClickHandler, this));
        this.selectByIndex(loadIndex);

        this.next = function() { 
            this.selectByIndex( ((this.currentIndex + 1) % this.elements.length));
          };

        this.prev = function() { 
            this.selectByIndex( ((this.currentIndex + this.elements.length - 1) % this.elements.length));
          };

      };

    ChooseList.prototype.onClickHandler = function(evt) {
        evt.preventDefault();
        this.selectByElement(evt.currentTarget);
      }

    ChooseList.prototype.selectByIndex = function(i) {
        this.selectByElement(this.elements[i]);
      };ChooseList.prototype.selectByElement = function(el) {
          if (this.selectedNode) {
              $j(this.selectedNode).removeClass("selected");
            };
          $j(el).addClass("selected");
          for (var i=0; i < this.elements.length; i++) {
              if (this.elements[i] === el) {
                  this.currentIndex = i;
                }
            };
          this.selectedNode = el;
          this.onChange(this);
        };

    function onMainChange(evt) {
        $j("#"+sections[tabIndex]+"_place").fadeOut('fast', function() {
            $j("#"+sections[evt.currentIndex]+"_place").fadeIn();
          });
        tabIndex = evt.currentIndex;
      }

    initialLoad = window.location.hash.substr(1);    
    mainNav = new ChooseList($j(".navigation_list"), onMainChange, initialLoad);

    $j(function(){
      $j("#include-publications").load("publications-generated.html");
    });

</script>
</body> 
</html>