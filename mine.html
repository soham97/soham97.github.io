<meta name="viewport" content="width=device-width, initial-scale=1">
<html> 
<head> 
  <link type="text/css"
        rel="stylesheet"
        href="style.css">
<title>Soham Deshmukh</title> 
</head> 
<body> 
<h1>Soham Deshmukh</h1> 
<div style="display:grid;grid-template-columns: 80% 20%;">
  <div style="float:left;">
    <p>
    I'm an Applied Scientist in the <a href="https://www.lti.cs.cmu.edu/">Speech team</a> at <a href="https://www.cs.cmu.edu/">Microsoft</a>, working on speech and audio processing. 
    </p>
    <p>
    My work focuses on building audio processing technology to reduce communication barriers and enabling seamless interactions. 
    This ranges from front-end audio processing like speech enhancement to building general purpose audio assistants.  
    In practice, my research enables voice-enabled Human-Machine Interactions using built-in microphones in today's devices.
    </p>
    <p>
    Previously, I received my masters degree from Carnegie Mellon University in <a href="https://cmu-mlsp.github.io/">MLSP Group</a> and advised by <a href="https://cmu-mlsp.github.io/team/bhiksha_raj">Bhiksha Raj</a>. 
    I completed my B.Tech from <a href="https://en.wikipedia.org/wiki/Veermata_Jijabai_Technological_Institute">VJTI</a>, working on NLP.
    </p>
    <b>Collaborations</b>: If you have questions or want to collaborate with me, feel free to email me.
    </p>

    <p>
    <b>Links:</b>
    <a href="https://scholar.google.com/citations?user=MasiEogAAAAJ&hl=en">Google Scholar</a> &bull;
    <a href="https://github.com/soham97">GitHub</a> &bull;  
    <a href="https://x.com/sohamdesh_">Twitter</a>
    </p>

  </div>
  <div style="float:left">  
    <img class="mug center" style="max-width:85%" src="soham.jpg" alt="Soham Deshmukh"> 
  </div>

</div>

<hr>
<div class="navigation_list">
  <h2>
    <a href="#research_place">Research</a> &bull;
    <a href="#updates_place">Updates</a> &bull;
    <!-- <a href="#people_place">People</a> &bull; -->
    <a href="#papers_place">Papers</a> &bull;
    <a href="#press_place">Press</a> &bull;
    <!-- <a href="#talks_place">Talks</a> &bull;
    <a href="#teaching_place">Teaching</a>  -->
    <a href="#outreach_place">Outreach</a>  -->
    <!-- &bull;<a href="#contact">Contact</a> -->
  </h2>
</div>

<div id="places">
<hr>

<div id="research_place">
  <h2>Research Highlights</h2>
    <br>
    <br>
  <table>
    <colgroup>
      <col style="max-width: 50%">
      <col style="max-width: 50%">
    </colgroup>
    <tbody>
    <tr>
      <td>
        <div class=highlights>
          <b>Front-end Audio Processing</b><br>
          <p>
            Audio processing on the edge/client side i.e. before audio is sent to the server for further processing. 
            This includes tasks such as speech enhancement, background estimation, echo cancellation, to facilitate clear voice-enabled interactions:
          </p>
          <ul class=collapsed class=bulletless>
            <li><a href=https://www.microsoft.com/en-us/edge/features/real-time-video-translation?form=MA13FJ>Real-time video translation</a></li>
            <li><a href=https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/announcing-video-translation-amp-speech-translation-api/ba-p/4148007>Video Translation API</a></li>
          </ul>
        </div>
      </td>
      <td>
        <img class="side-image" src="images/translation.png">
      </td>
    </tr>

    <tr>
      <td>
        <div class=highlights>
          <b>Audio-Language Models</b><br>
          <p>
            Grounding the LLMs in audio modality will enable models which can perceive (hear), analyze, and interact with users through natural language. By grounding language in audio, we aim to learn a rich audio-text representation that can enable various multimodal applications like text queried- retrieval, captioning, generation and question-answering
          </p>
          <ul class=collapsed class=bulletless>
            <li><a href=https://arxiv.org/abs/2309.05767>Natural Language Supervision for General-Purpose Audio Representations</a> (ICASSP 2024)</li>
            <li><a href=https://arxiv.org/abs/2212.10481>Pengi üêß: An Audio Language Model for Audio Tasks</a> (NeurIPS 2024)</li>
            <li><a href=https://arxiv.org/abs/2206.04769>CLAP üëè: Learning Audio Concepts From Natural Language Supervision</a> (ICASSP 2023)</li>
          </ul>
        </div>
      </td>
      <td>
        <img class="side-image" src="images/alm2.png">
      </td>
    </tr>
    </tbody>
  </table>

</div>


<div id="updates_place">
<h2>News</h2>
  <ul class="collapsed bulletless">
    <li> <b>May 2024:</b> Helping organize a <a href="https://llmagents.github.io/">Workshop on LLM Agents</a> at ICLR 2024. </li>
    <li> <b>Mar 2024:</b> Helping organize <a href="https://unimplicit2024.github.io/">UnImplicit: The 3rd Workshop on Understanding Implicit and Underspecified Langauge</a> at EACL 2024. </li>
    <li> <b>Dec 2023:</b> Talk at the <a href="https://www.cs.mcgill.ca/~pparth2/nilli_workshop_2023/">NILLI workshop</a> at EMNLP 2023 on <a href="talks/interacting-with-llms.pdf"><i>Interacting with LLMs for Grounded Tasks</i></a>. <a href="https://cmu.box.com/s/hngwix4w7x5a87kf5kxgwn9sthkm6vb3">(video)</a>. </li>
    <li> <b>Nov 2023:</b> Talk at the <a href="https://www.neurosymbolic.org/reading-group.html">MIT Neurosymbolic Reading Group</a> on <a href="talks/interacting-code-llms.pdf"><i>Interacting with (Code) LLMs</i></a>. </li>
    <li> <b>Sept 2023:</b> Honored to receive an <a href="http://www.okawa-foundation.or.jp/en/activities/research_grant/list.html">Okawa Research Grant</a> for our work on pragmatics and language interfaces.
    <li> <b>Spring 2023:</b> Invited talks at GitHub Next and Bloomberg AI on <a href="talks/starcoder_slides.pdf"><i>InCoder, SantaCoder, and StarCoder: Findings from Training Open-Source Code LLMs</i></a>.
    <li> <b>Spring 2023:</b> <a href="https://huggingface.co/bigcode/starcoder">StarCoder code generation models</a> released with the <a href="https://huggingface.co/bigcode">BigCode project</a>. Check out the <a href="https://arxiv.org/abs/2305.06161">paper</a>, <a href="https://huggingface.co/spaces/bigcode/bigcode-editor">demo</a>, and <a href="https://marketplace.visualstudio.com/items?itemName=HuggingFace.huggingface-vscode">VSCode extension</a>.</li>
    <li> <b>Spring 2023:</b> Invited talks at UT Austin, JHU, and UPenn on <a href="https://www.youtube.com/watch?v=qiA0-JrEobo"><i>Using Language Strategically in Context</i></a> (<a href="https://www.dropbox.com/s/fi2w9q44xsjlps7/2023-using-language-prd.pdf?dl=0">slides</a>). </li>
    <li> <b>Dec 2022:</b> Invited talk at <a href="https://sites.google.com/view/corl-2022-games-workshop/home">CoRL Workshop on Strategic Multi-Agent Interactions</a>.</li>
    <li> <b>Nov 2022:</b> Preprint of our survey and position paper on <a href="https://arxiv.org/abs/2211.08371">Pragmatics in Language Grounding</a>. </li>
    <li> <b>Nov 2022:</b> <a href="https://www.science.org/doi/10.1126/science.ade9097">Paper in Science</a> with the FAIR Diplomacy team on an agent for human-level play in Diplomacy, a dialogue- and negotiation-based strategy game. <a href="https://www.science.org/content/article/ai-learns-art-diplomacy-game">Overview article.</a> </li>
    <li> <b>Fall 2022:</b> Several preprints with collaborators on language-to-code generation: models (<a href="https://sites.google.com/view/incoder-code-models/home">InCoder</a>, <a href="https://huggingface.co/bigcode/santacoder">BigCode project</a>), 
      inference techniques (<a href="https://arxiv.org/abs/2204.11454">MBR-Exec</a>, <a href="https://arxiv.org/abs/2211.16490">Coder-Reviewer Reranking</a>), and benchmark datasets (<a href="https://ds1000-code-gen.github.io/">DS-1000</a>, <a href="https://arxiv.org/abs/2212.10481">ODEX</a>).
      <li> <b>Sep 2022:</b> Invited talk at LTI "Future of Code Generation" Seminar on <a href="talks/programming-communication.pdf"><i>Contextual Communication in Programming</i></a>. </li>
    <li> <b>Aug 2022:</b> Joined CMU!</li>
    <li> <b>Jul 2022:</b> Co-organized NAACL <a href="https://unimplicit2022.github.io/">Workshop on Implicit and Underspecified Language</a> </li>
    <li> <b>Apr 2020:</b> Voice-based COVID-19 screening is covered in <a href="https://www.forbes.com/sites/marcwebertobias/2020/05/05/ai-and-medical-diagnostics-can-a-smartphone-app-detect-covid-19-from-speech-or-a-cough/#7a9114225436">Forbes</a>, <a href="https://gizmodo-com.cdn.ampproject.org/c/s/gizmodo.com/researchers-built-an-app-that-aims-to-detect-covid-19-b-1842613139/amp">Gizmodo</a></i>.</li>
  </ul>
</div>
<hr>

<div id="papers_place">
<h2>Papers</h2>

<ul class="bulletless">
  <div id="include-publications"></div>
</ul>

</div>
<hr>

<div id="talks_place">
<h2>Talks</h2>
<ul class="bulletless">
  <!-- <li>Short overview: <a href="https://www.youtube.com/watch?v=psZYprYXygc">video (6 minutes)</a></li> -->
  <li>Short overview: <a href="https://cmu.box.com/s/dv1nrsokedught89t03oikof0lwevlr6">video (7 minutes)</a></li>
  <li>Interacting with LLMs for Grounded Tasks: <a href="https://cmu.box.com/s/hngwix4w7x5a87kf5kxgwn9sthkm6vb3">video (45 minutes)</a>; <a href="talks/interacting-with-llms.pdf">slides</a> </li>
  <li>Using language strategically in context: <a href="https://www.youtube.com/watch?v=qiA0-JrEobo">video (1 hour)</a>; <a href="https://www.dropbox.com/s/fi2w9q44xsjlps7/2023-using-language-prd.pdf?dl=0">slides</a> </li>
  <li>Code generation as communication: <a href="talks/programming-communication.pdf">slides</a> </li>
  <!--
  <li>Thesis work on pragmatics: <a href="https://www.youtube.com/watch?v=psZYprYXygc">6 min; 2022</a> and <a href="https://cmu.box.com/s/of5v67nxaloio5ngnuk31wtxcl5fopoa">45 min; 2021</a> </li>
  -->
</ul>
</div>

<hr>

<div id="teaching_place">
<h2>Teaching</h2> 

<h3> Courses </h3>

<ul class="paddedList">
    <li><strong>Multimodal Machine Learning (11-777)</strong>. Fall 2024. </li>
  <li><strong><a href="https://cmu-multicomp-lab.github.io/adv-mmml-course/spring2024/">Advanced Multimodal Machine Learning (11-877)</a></strong>. Spring 2024. </li>
    <li><strong><a href="https://cmu-codegen.github.io/s2024/">Neural Code Generation (11-891)</a></strong>. Spring 2024. </li>
    <li><strong><a href="https://cmu-anlp.github.io/">Advanced Natural Language Processing (11-711)</a></strong>. Fall 2023. </li>
    <li><strong><a href="https://cmu-mmml.github.io/spring2023/">Multimodal Machine Learning (11-777)</a></strong>. Spring 2023. </li>

      <!--
    <li><strong>Co-instructor, Introduction to Artificial Intelligence (CS 188)</strong>. <br>
    UC Berkeley, Summer 2018.

    <li><strong>Teaching assistant, Introduction to Artificial Intelligence (CS 188)</strong>. <br>
    UC Berkeley, Fall 2017.

    <li><strong>Teaching assistant, Introduction to Discrete Structures (CS 245)</strong>. <br>
    University of Arizona, Spring 2012.

    <li><strong>Teaching assistant, Great Ideas of the Information Age (ISTA 100)</strong>. <br>
    University of Arizona, Fall 2011.
      -->
</ul>

<h3> Course Materials </h3>
<ul class="paddedList">
  <li> 
    <a href="https://sites.google.com/view/nlp-assignments">Interactive Assignments for Teaching Structured Neural NLP</a>: assignments we developed for UC Berkeley's <a href="http://nlp.cs.berkeley.edu/teaching.shtml">graduate NLP course (CS 288)</a>. They teach structured prediction using a combination of modern neural architectures and classic inference algorithms (in PyTorch and CoLab).
  </li>

  <li>
    <a href="https://www.dropbox.com/s/zqd5btwr7oqhxz7/grounding-sp21.pptx?dl=0">Grounded Semantics</a> lecture, with thanks to Greg Durrett and Chris Potts for slides.
  </li>

  <li>
    <a href="lectures/game-theoretic-pragmatics.pdf">Pragmatic Language Games</a> lecture, with thanks to Nick Tomlin for slides.
  </li>
</ul>

</div>
<hr>
<!--
<div id="contact_place">
  <h2>Contact Info</h3>
</div>
-->

</div> <!-- end places -->

<script src="static/jquery.min.js"></script>
<script>
    var $j = jQuery;

    var tabIndex = 0;

    var sections = ["research", "news", "people", "papers", "talks", "teaching"];

    for(i=0; i<sections.length; i++) {
        $j("#"+sections[i]+"_place").hide();
      }

    $j('#places').find('hr').hide();
    $j('#places').find('h2').hide();


    ChooseList = function(node, changeCallback, place) {
        loadIndex = sections.indexOf(place) == -1 ? 0 : sections.indexOf(place);

        this.container = $j(node);
        this.selectedNode = null;
        this.currentIndex = null;
        this.onChange = changeCallback;
        this.elements = this.container.find('a');
        this.elements.removeAttr('target');
        this.container.find('a').on('click', $j.proxy(this.onClickHandler, this));
        this.selectByIndex(loadIndex);

        this.next = function() { 
            this.selectByIndex( ((this.currentIndex + 1) % this.elements.length));
          };

        this.prev = function() { 
            this.selectByIndex( ((this.currentIndex + this.elements.length - 1) % this.elements.length));
          };

      };

    ChooseList.prototype.onClickHandler = function(evt) {
        evt.preventDefault();
        this.selectByElement(evt.currentTarget);
      }

    ChooseList.prototype.selectByIndex = function(i) {
        this.selectByElement(this.elements[i]);
      };ChooseList.prototype.selectByElement = function(el) {
          if (this.selectedNode) {
              $j(this.selectedNode).removeClass("selected");
            };
          $j(el).addClass("selected");
          for (var i=0; i < this.elements.length; i++) {
              if (this.elements[i] === el) {
                  this.currentIndex = i;
                }
            };
          this.selectedNode = el;
          this.onChange(this);
        };

    function onMainChange(evt) {
        $j("#"+sections[tabIndex]+"_place").fadeOut('fast', function() {
            $j("#"+sections[evt.currentIndex]+"_place").fadeIn();
          });
        tabIndex = evt.currentIndex;
      }

    initialLoad = window.location.hash.substr(1);    
    mainNav = new ChooseList($j(".navigation_list"), onMainChange, initialLoad);

    $j(function(){
      $j("#include-publications").load("publications-generated.html");
    });

</script>
</body> 
</html>
